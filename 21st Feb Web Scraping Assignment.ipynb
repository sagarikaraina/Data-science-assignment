{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede78eb7-ed3e-42e4-afd0-5a134d8caa56",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eee414-e028-44b8-9efa-b45183857dff",
   "metadata": {},
   "source": [
    "* Web scraping is the process of extracting data from websites using software tools. \n",
    "* It involves fetching and parsing the HTML code of a web page to extract relevant data, such as text, images, links, and other structured data.\n",
    "* Used to automate data collection tasks that would otherwise be time-consuming or impossible to perform manually.\n",
    "\n",
    "Web scraping is used in many areas, including:\n",
    "\n",
    "1. E-commerce: Web scraping is used to extract product information, pricing data, and customer reviews from e-commerce websites to monitor competitors, track pricing trends, and inform pricing and product strategies.\n",
    "\n",
    "2. Market Research: Web scraping is used to collect data on consumer sentiment, trends, and behavior from social media, online forums, and other sources to inform market research and business decision-making.\n",
    "\n",
    "3. Journalism: Web scraping is used by journalists to gather information and data for stories, such as public records, court documents, and other publicly available information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503eed94-bea1-4716-a193-61b19d771955",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47999a-2566-4320-8d50-456c36c172d1",
   "metadata": {},
   "source": [
    "1. Parsing HTML: This is the most basic method of web scraping, where the HTML code of a web page is fetched and parsed to extract relevant data. This method involves using a programming language like Python, along with libraries like BeautifulSoup to parse the HTML code and extract the desired data.\n",
    "\n",
    "2. Automated browsing: This method involves using a web browser automation tool like Selenium to interact with the website.The automation tool navigates the website, clicks links, fills in forms, and extracts data from the resulting web pages. This method can be useful when the website uses dynamic content to display data.\n",
    "\n",
    "3. API access: Some websites provide APIs (Application Programming Interfaces) that allows us to access data in a structured format. This method involves making requests to the API endpoints using programming languages like Python, and parsing the returned data.\n",
    "\n",
    "4. Machine learning: Machine learning algorithms can also be used for web scraping tasks. This method involves training a machine learning model to recognize and extract relevant data from web pages. However, this method requires a large amount of labeled training data and can be computationally expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1996afc2-2295-4733-9c5c-01b9b444e750",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df21d7-e79e-4cb4-973e-077a6fcee2c3",
   "metadata": {},
   "source": [
    "* Beautiful Soup is a powerful and flexible library that makes web scraping tasks easier and more efficient. \n",
    "* Allows quick and easy extraction of data from web pages, and integrate that data into other Python projects and workflows.\n",
    "\n",
    "Beautiful Soup is used for several reasons:\n",
    "\n",
    "1. Easy to Use: Beautiful Soup is easy to learn and use, even for beginners with little to no programming experience. It has a simple syntax that allows developers to quickly parse and extract data from web pages.\n",
    "\n",
    "2. Parsing HTML and XML: Beautiful Soup can parse both HTML and XML documents. It provides a robust and flexible way to extract data from web pages, even when the HTML is poorly formatted or not well-structured.\n",
    "\n",
    "3. Navigating the DOM: Beautiful Soup allows developers to navigate the Document Object Model (DOM) of a web page to locate and extract data. It provides a range of methods for searching and traversing the DOM, including finding elements by tag name, class name, attribute, and more.\n",
    "\n",
    "4. Data Extraction: Beautiful Soup provides a range of methods for extracting data from web pages, including text, links, images, and other structured data. It also allows developers to extract data from nested elements and attributes, and to handle exceptions and errors.\n",
    "\n",
    "5. Integration with Other Libraries: Beautiful Soup can be easily integrated with other Python libraries like Requests for making HTTP requests, Pandas for data analysis, and Scrapy for web scraping tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e6fae-2af2-45aa-9781-7230e195e5ea",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484dc46-7742-4ee0-ae0e-2e02a319d339",
   "metadata": {},
   "source": [
    "1. Serve as a Web API: Flask can be used to create a web API that can receive requests from other applications or scripts, process those requests, and return responses in a structured format like JSON or XML. This can be useful for integrating the web scraping project with other applications or services.\n",
    "\n",
    "2. Handle HTTP Requests: Flask provides an easy way to handle HTTP requests like GET, POST, and DELETE. This can be useful in a web scraping project where data needs to be fetched from web pages and processed according to specific requirements.\n",
    "\n",
    "3. Handle Data: Flask can be used to handle data returned from web scraping tasks. This includes data cleaning, processing, and storing in a database or other data storage system.\n",
    "\n",
    "4. Visualize Data: Flask can be used to create data visualization tools that can help users to understand the results of web scraping tasks. This can include creating charts, graphs, and other visualizations based on the scraped data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb0497-ca37-4b7a-8888-42a8c74a711f",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8d0bf-c374-476c-af4d-41e93d3a525a",
   "metadata": {},
   "source": [
    "Cloud pipeline is a fully managed continuous delivery service that automates the build, test, and release process for applications. \n",
    "It provides a way to create and manage pipelines that automate the deployment of code changes to production environments. \n",
    "while Elastic Beanstalk is a PaaS service for simplifying the deployment and management of web applications in the cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
